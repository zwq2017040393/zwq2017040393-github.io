<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>交叉熵损失函数的学习</title>
      <link href="/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在分类问题中，交叉熵损失函数常常被应用，然而作为初学者，我却对它了解非常浅薄，希望借着开通博客的热情，再一次好好学一下这个函数。<br>我计划将通过信息论的知识引入，再逐渐推导出交叉熵函数的公式，明白它的意义，再尽量理解为什么分类问题更多的用交叉熵损失函数。</p></blockquote><p style="text-indent: 25px">先参考《深度学习》花书中关于的信息论的描述：信息论的基本想法是一个不太可能的事情居然发生了，要比一个非常可能的事件发生，能提供更多的信息。消息说：“今天早上太阳升起”，信息量是如此之少，以至于没有必要发送；但一条消息说：“今天早上有日食”，信息量就很丰富。</p><p style="text-indent: 25px">我们想要通过这种基本想法量化信息。特别是：</p><ul style="background-color: silver;position: center;left: 5cm"><li style="color: #3e3e3e">非常可能发生的事情信息量要比较少，并且极端情况下，确保能够发生的事情应该没有信息量。</li><li style="color: #3e3e3e">较不可能发生的事情具有更高的信息量。</li><li style="color: #3e3e3e">独立事件应具有增量的信息。例如投掷的硬币两次正面向上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍</li></ul><p style="text-indent: 25px">为了满足上述3个性质，我们定义一个事件x =$x$的<b>自信息</b>(self-attention)为:</p><p style="text-align: center"><span>$I(x)=-logP(x)$</span><span style="padding-left: 10%">(公式1)</span></p><p>(其中log为自然对数，底数为自然数e)。因此我们定义的$I(x)$单位是奈特(nats)。一奈特是以$\frac{1}{e}$的概率观测到一个事件时获得的信息量。若有材料中使用底数为2的对数，那么它的单位是比特(bits)或者香农(shannons);通过比特度量的信息只能是通过奈特度量信息的常数倍。</p><img src="/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/y_log(x).png" , height="50%" width="50%" align="center"><p style="font-size: small;text-align: center;margin-top: 0">图1.y=-log(x)图像 由图像知：x发生的概率的越大，所拥有的信息量越少。</p><p style="text-indent: 25px">当x是连续的，我们使用类似的关于信息的定义，但有些来源于离散形式的性质就丢失了。例如，一个具有单位密度的事件信息量仍然为0，但是不能保证它一定发生。(据我理解，应该是概率为1的事件也不一定发生吧)。</p><p style="text-indent: 25px">自信息只能处理单个的输入。我们可以用<b>香农熵</b>(Shannon entropy)来对整个概率分布中的不确定性总量进行量化:</p><p style="text-align: center">$H(x)=\mathbb{E}_{x\sim P}\left[ I(x)\right] = -\mathbb{E}_{x\sim P}\left[ logP(x)\right]$<span style="margin-left:10% ">(公式2)</span></p><p>也记作$H(P)$,换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义上的下界(当对数底数不是2时，单位将有所不同)。那些接近确定性的分布（输出几乎可以确定）具有较低的熵；那些接近均匀分布的概率分布具有较高的熵。（这句话想了好久才理解，可以参考这位大神的讲解<a href="https://zhuanlan.zhihu.com/p/67490724">【机器学习】香农熵的一些知识点</a>）图2给出了一个说明。当x是连续的，香农熵被称为<b>微分熵</b>(differential entropy)。</p><img src="/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/XNS.jpg" , width="80%" height="100%" align="center"><p style="font-size: small">图2.二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更接近均匀分布的分布是如何具有较高的香农熵。水平轴是p，表示二值随机变量等于1的概率。熵由$(p-1)log(1-p)-plog(p)$给出。当p接近0时，分布几乎是确定的，因为随机变量几乎总是0.当p接近1时，分布几乎是确定的，因为随机变量几乎总是1。当p=0.5时，熵是最大的，因为分布在两个结果(0和1)上是均匀的。</p><p style="text-indent: 25px">如果对于同一个随机变量x具有两个单独的概率分布P(x)和Q(x)，可以使用<b>KL散度</b>(Kullback(KL) divergence)来衡量这两个分布的差异:</p><p style="text-align: center">$D_{KL}\left( P||Q\right) = \mathbb{E}_{x\sim P}\left[ log\frac {P(x)}{Q(x)}\right] = \mathbb{E}_{x\sim P}\left[ logP(x)-logQ(x)\right]$<span style="margin-left: 10%">(公式3)</span></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 损失函数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 损失函数 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义分割经典论文小结</title>
      <link href="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/"/>
      <url>/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>导读：图像的语义分割是计算机视觉的基本任务之一，旨在将图像中的每一个像素进行分类。相对于传统分隔方法，从FCN开始的以深度学习方法为主的端到端语义分割模型将精度带来了质上的提升，<br>本文将最近看过的语义分割模型进行一个总结，旨在在今后的学习中，能够更快更好得回忆起曾经看过的论文，找到思路。（毕竟只是小结，细节不是那么全面）</p></blockquote><h3 id="1-开山之作：FCN全卷积神经网络"><a href="#1-开山之作：FCN全卷积神经网络" class="headerlink" title="1. 开山之作：FCN全卷积神经网络"></a>1. 开山之作：FCN全卷积神经网络</h3><blockquote><p>论文题目《Fully Convolutional Networks for Semantic Segmentation》<br>论文地址:<a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a><br><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-0.png" width="60%" height="60%" div align="center"></p></blockquote><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在检测任务中，通常对图片产生一定量的候选区域，放入卷积神经网络中进行分类，卷积神经网络在卷积之后会接上全连接层，将产生的特征矩阵（二维）压缩为一维向量，全连接层最后一层输出概率向量，概率最大的就是当前目标的类别。但是在语义分割中这么做有明显的缺点：语义分割需要对每一个像素分类，若采用前面的方法， 会比检测问题多的多的候选区域，无疑极度增加了计算量和存储需求；其次，候选区域的大小不易确定；再者，网络的感知域受到候选区域大小的限制，不能很好地利用上下文信息。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该论文开创性地提出了一种end-to-end的全卷积网络训练方式，使卷积神经网络方法可以现实的进行语义分割。</p><h4 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图1-1所示，FCN相较于CNN的不同之处，是讲网络后面的全连接层替换成为了卷积层，最后一层的输出由一条线（概率向量）变为了一个二维热图（实际上是三维输出，每个像素有属于不同类别的概率，给输出增添了一维）。</p><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-1.png" width="80%" height="80%" div align="center"><div style="font-size: small; text-align: center">图1-1.FCN与CNN</div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   将特征矩阵从粗糙（coarse outputs）变回像素级别的预测(pixel-dense outputs)是关键，这个过程一般称为上采样（upsample），常见的上采样方法有：转置卷积（反卷积upconv）、插值法（interpolation）、反池化(unpool)，该论文中采用的方法是双线性插值（bilinearly upsample），选择此方法的原因忘了，也不想在论文中去找，但我估计这种方法的优点是实现起来相对简单，计算量小一些。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;保留CNN的卷积（conv1-conv4）将fc层（full connection,全连接）替换去掉后，对之前的最后一层卷积继续采用卷积操作（记为conv5，卷积核的大小不一定和论文中一样，本人理解可以自己选择适当的大小），通过控制通道数将得到的特征通道数增加到与原本的全连接层节点数相同的数目，再加一次池化，再采用一次1x1卷积进训练，（这次不改变通道数，可以记为conv6），然后进行1x1卷积，并将输出通道变为像素类别数（类别加背景）。最后通过上采样将分辨率调整到目标大小（原图分辨率）。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于经历了多层卷积操作，最后得到的特征是高语义，低精度的，为了减少反卷积得到的pixelwise prediction的精度损失，我们可以利用不同卷积层提取的出的不同的层级特征。如图1-2，论文中采用了skip architecture,该结构使粗糙的深层特征与精度高的浅层特征融合，来产生更准确、精细的结果。</p><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-2.png" width="80%" height="80%" div align="center"><div style="font-size: small; text-align: center">图1-2.FCN的skip structure</div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将最后得到的最小尺寸的的特征图（这里记作mini-feature吧）直接进行步长为32的上采样，被称为FCN-32s；将最后得到的mini-feature进行2x2上采样，将结果与pool4后得到的feature_map融合（得到的结果记为mini2_-feature吧），再进行步长为16的上采样，被称为FCN-16s；将mini2_feature进行2倍的上采样，与pool3得到的feature_map融合，再进行步长为8的上采样，得到原图的分辨率，记为FCN-8s。（对特征融合的具体实现过程，我始终有着疑惑：既然不同层pool的通道数不同，那融合之后通道数应该与目标通道数不同，是否该做额外的处理，该如何处理，还没看到论文说，看样子只有看源代码才能解决问题了！）</p><h4 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="(3) 实验结果"></a>(3) 实验结果</h4><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-3.png" width="60%" height="60%" div align="center"><div style="font-size: small; text-align: center">图1-3的skip structure</div>emmmm...到了实验这块，论文我就没有好好看，实际上实验也是十分重要的，这篇论文比较详细地给出了模型的构建过程，以后也可以细看看。这篇论文的做的实验太多了，不往博客写了，还是看论文吧。将实验结果总结起来，有如下结论：<blockquote><p>1.调整了AlexNet，VGG16，GoogleNet等3个分类网络，得出backbone使用VGG16的效果最好（那时候还没有resnet和densenet，要不我想结果会改变的）<br>2.FCN-8s的效果好于FCN-16s和FCN-32s<br>3.对整幅图训练和对50%、25%采样后的图训练，最后得到的loss是差不多的，但是对整幅图训练loss下降得快一些。</p></blockquote><h3 id="2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation"><a href="#2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="2. U-Net:Convolution Networks for Biomedical Image Segmentation"></a>2. U-Net:Convolution Networks for Biomedical Image Segmentation</h3><blockquote><p>论文题目：《U-Net:Convolution Networks for Biomedical Image Segmentation》<br>论文地址：<a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p></blockquote><h4 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在医学图像处理中，常常缺少足够的标记数据 。Ciresan等人训练了一个利用滑动窗口来预测像素类别的分割网络，这种网络通过在像素周围产生一个区域local region（patch）作为输入。这种网络既能给像素定位，也能产生更多的训练数据。但这种方法有两个明显的缺点，第一：慢；第二：在位置精度与上下文的使用具有矛盾（用自己的话说：高语义低精度和高精度低语义之间的矛盾)。这篇论文中，在FCN为基础上，提出了U-net。</p><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet1.png" , align="center" height="80%" width="80%"><div style="font-size: small;text-align: center">图2-1.论文中U-net的网络图 </div><h4 id="2-模型结构-1"><a href="#2-模型结构-1" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图2-1所示，相较于FCN一个大的不同，是在上采样中U-net使用了大量的特征通道，在最后一步中才将通道数降为类别数，这样在传播中能够为高语义部分保留更多的上下文信息。该网络形似一个字母'U',这也是它的名字的由来吧！而且我们可以看到，在expansive path中通过skip connection融合了来自contracting path中对应层级的特征。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;医学图片一般分辨率很大，所以我们往往将其切割成许多小的图片，在切割过程中，为了能让图片边界也能“享有”从分利用上下文信息的权利，论文采用了一种称作“overlap-tile”的方法，如图2-2所示，对原图的边界部分采用了对称策略（镜像拷贝），将原图扩大一些，这样就能增加一些边界部分像素的上下文信息了。（老师曾布置给我一个遥感图像变化检测任务，对于原分辨率的图像我也是这么处理的，这么一想，我也做了一定程度的创新，嘿嘿）。</p><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet2.png" , align="center" width="80%" height="80%"><div style="text-align: center;font-size: small">图2-2. Overlap-tile</div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了一定程度上解决缺乏数据量的问题，可以将图片进行数据增强，除了传统的数据增强方法外，还可以针对医学图像的特殊性质而安排的增强方式：弹性增强。顾名思义，由于细胞是有弹性的，所以我们可以对图片和其标记做一些弹性的形变，放入网络中进行训练。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一个挑战是相同类别的细胞总是接触到了一起，这就让细胞之间的边界难以被识别，论文采用了带权损失，在相互接触的两个细胞之间的连接（边界）部分获得更高的权重，如图2-3所示。</p><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet3.png" , height="80%" width="80%" align="center"><div style="text-align: center;font-size: small">图2-3</div><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="(3) 实验"></a>(3) 实验</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于论文较为古老，所以实验结果没有细看，大概就是在各种医学图像分割的结果中，都碾压了当时的其他模型吧。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;U-net模型十分轻量，在图像分割使上具有十分重大的意义，之后的许多模型都是基于U-net进行魔改，确实是为图像分割开创了一个新的篇章。</p><h3 id="3-《deeplabv1》空洞卷积"><a href="#3-《deeplabv1》空洞卷积" class="headerlink" title="3.《deeplabv1》空洞卷积"></a>3.《deeplabv1》空洞卷积</h3><div style="font-size: xxx-large">未完待续！！！</div>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 图像语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/07/06/hello-world/"/>
      <url>/2021/07/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 试验 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
