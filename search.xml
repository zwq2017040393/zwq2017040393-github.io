<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/07/06/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>交叉熵损失函数的学习</title>
    <url>/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<blockquote>
<p>在分类问题中，交叉熵损失函数常常被应用，然而作为初学者，我却对它了解非常浅薄，希望借着开通博客的热情，再一次好好学一下这个函数。<br>我计划将通过信息论的知识引入，再逐渐推导出交叉熵函数的公式，明白它的意义，再尽量理解为什么分类问题更多的用交叉熵损失函数。</p>
</blockquote>
<p style="text-indent: 25px">
先参考《深度学习》花书中关于的信息论的描述：信息论的基本想法是一个不太可能的事情居然发生了，要比一个非常可能的事件发生，能提供更多的信息。
消息说：“今天早上太阳升起”，信息量是如此之少，以至于没有必要发送；但一条消息说：“今天早上有日食”，信息量就很丰富。
</p>
<p style="text-indent: 25px">
我们想要通过这种基本想法量化信息。特别是：</p>
<ul style="background-color: silver;position: center;left: 5cm">
<li style="color: #3e3e3e">非常可能发生的事情信息量要比较少，并且极端情况下，确保能够发生的事情应该没有信息量。</li>
<li style="color: #3e3e3e">较不可能发生的事情具有更高的信息量。</li>
<li style="color: #3e3e3e">独立事件应具有增量的信息。例如投掷的硬币两次正面向上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍</li>
</ul>
<p style="text-indent: 25px">
为了满足上述3个性质，我们定义一个事件x =$x$的<b>自信息</b>(self-attention)为:</p>
<p style="text-align: center"><span>$I(x)=-logP(x)$</span><span style="position: absolute;right:0 ">(公式1)</span></p>
<p>
(其中log为自然对数，底数为自然数e)。因此我们定义的$I(x)$单位是奈特(nats)。
一奈特是以$\frac{1}{e}$的概率观测到一个事件时获得的信息量。若有材料中使用底数为2的对数，那么它的单位是比特(bits)或者香农(shannons);通过比特度量的信息只能是通过奈特度量信息的常数倍。</p>
<img src="/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/y_log(x).png" , height="50%" width="50%" align="center">
<p style="font-size: small;text-align: center;margin-top: 0">图1.y=-log(x)图像 由图像知：x发生的概率的越大，所拥有的信息量越少。</p>
<p style="text-indent: 25px">
当x是连续的，我们使用类似的关于信息的定义，但有些来源于离散形式的性质就丢失了。例如，一个具有单位密度的事件信息量仍然为0，但是不能保证它一定发生。(据我理解，应该是概率为1的事件也不一定发生吧)。
</p>
<p style="text-indent: 25px">
自信息只能处理单个的输入。我们可以用<b>香农熵</b>(Shannon entropy)来对整个概率分布中的不确定性总量进行量化:
</p>
<p style="text-align: center">$H(x)=\mathbb{E}_{x\sim P}\left[ I(x)\right] = -\mathbb{E}_{x\sim P}\left[ logP(x)\right]$<span style="position: absolute;right:0  ">(公式2)</span></p>
<p>也记作$H(P)$,换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义上的下界(当对数底数不是2时，单位将有所不同)。
那些接近确定性的分布（输出几乎可以确定）具有较低的熵；那些接近均匀分布的概率分布具有较高的熵。（这句话想了好久才理解，可以参考这位大神的讲解<a href="https://zhuanlan.zhihu.com/p/67490724">【机器学习】香农熵的一些知识点</a>）
图2给出了一个说明。当x是连续的，香农熵被称为<b>微分熵</b>(differential entropy)。
</p>
<img src="/2021/07/10/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0/XNS.jpg" , width="80%" height="100%" align="center">
<p style="font-size: small">图2.二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更接近均匀分布的分布是如何具有较高的香农熵。
水平轴是p，表示二值随机变量等于1的概率。熵由$(p-1)log(1-p)-plog(p)$给出。当p接近0时，分布几乎是确定的，因为随机变量几乎总是0.当p接近1时，分布几乎是确定的，因为随机变量几乎总是1。
当p=0.5时，熵是最大的，因为分布在两个结果(0和1)上是均匀的。</p>
<p style="text-indent: 25px">
如果对于同一个随机变量x具有两个单独的概率分布$P(x)$和$Q(x)$，可以使用<b>KL散度</b>(Kullback(KL) divergence)来衡量这两个分布的差异:
</p>
<p style="text-align: center">$D_{KL}\left( P||Q\right) = \mathbb{E}_{x\sim P}\left[ log\frac {P(x)}{Q(x)}\right] = \mathbb{E}_{x\sim P}\left[ logP(x)-logQ(x)\right]$<span style="position: absolute;right:0 ">(公式3)</span></p>
<p style="text-indent: 25px">
在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够使得概率分布Q产生的消息的长度最小的编码，发送包含由概率分布P产生的符号的信息时，所需要的额外信息量
（若果我们使用底数为2的对数时，信息量用比特衡量，但在机器学习中，我们通常用奈特和自然对数。更通俗的解释是：KL散度描述的是两个概率分布之间的差异） 
我再把上面的式子在拆一下吧，便于理解。得到的公式如公式4(假设是离散型随机变量)。
</p>
<p style="text-align: center">$D_{KL}(P||Q) = \mathbb{E}_{x\sim P}\left[ logP(x)-logQ(x)\right] = \sum_{x=1}^{n}\left[ p(x_i)log{p(x_i)} - p(x_i)log{q(x_i)}\right]$<span style="display: block;position: absolute;right:0 ">(公式4)</span></p>
<p style="text-indent: 25px">
由上式可以看出，当P和Q的概率分布相同时,$D_{KL}(P||Q)=0$,即两个概率之间KL散度（也叫相对熵）为0，当分布不一样时，KL散度大于0。
</p>
<p style="text-indent: 25px">
KL散度有很多有用的性质，最重要的是，它是非负的。KL散度是0，当且仅当P和Q在离散型变量的情况下是相同的分布，或者是在连续型变量的情况下是“几乎处处”相同的。
因为KL散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。然而，它并不是真的距离，因为它不是对称的：对于某些P和Q，
$D_{KL}(P||Q) \neq D_{KL}(Q||P)$。这种非对称性意味着选择$D_{KL}(P||Q)$还是$D_{KL}(Q||P)$影响很大。
</p>
<p style="text-indent: 25px">
一个和KL散度密切联系的量是<b>交叉熵</b>(cross-entropy),即$H(P,Q) = H(P)+D_{KL}(P||Q)$，它和KL散度很像，但是缺少左边一项：
</p>
<p style="text-align: center">$H(P,Q) = -\mathbb{E}_{x\sim P}logQ(x)$<span style="position: absolute;right:0 ">（公式5）</span></p>
<p>针对Q最小化交叉熵等价于最小化KL散度，因为Q并不参与被省略的那一项。由于$\sum_{i=1}^np(x_i)q(x_i) = \sum_{i=1}^nq(x_i)p(x_i)$，所以$H(P,Q) = H(Q,P)$
，所以对于P和Q选择的顺序并不影响交叉熵的结果，若当前已知的概率分布是P，由于交叉熵只是失去了KL散度的关于P的部分，所以损失的对于Q可以看做常数，则交叉熵也是反应两个分布之间的距离，所以可以用来用作深度学习的损失函数。
交叉熵常常与softmax函数是标配，先使预测的概率和为1，再和原本的概率分布一起计算损失。
</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>损失函数</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>损失函数</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>语义分割经典论文小结</title>
    <url>/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>导读：图像的语义分割是计算机视觉的基本任务之一，旨在将图像中的每一个像素进行分类。相对于传统分隔方法，从FCN开始的以深度学习方法为主的端到端语义分割模型将精度带来了质上的提升，<br>本文将最近看过的语义分割模型进行一个总结，旨在在今后的学习中，能够更快更好得回忆起曾经看过的论文，找到思路。（毕竟只是小结，细节不是那么全面）</p>
</blockquote>
<h3 id="1-开山之作：FCN全卷积神经网络"><a href="#1-开山之作：FCN全卷积神经网络" class="headerlink" title="1. 开山之作：FCN全卷积神经网络"></a>1. 开山之作：FCN全卷积神经网络</h3><blockquote>
<p>论文题目《Fully Convolutional Networks for Semantic Segmentation》<br>论文地址:<a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a><br><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-0.png" width="60%" height="60%" div align="center"></p>
</blockquote>
<h4 id="1-简介"><a href="#1-简介" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在检测任务中，通常对图片产生一定量的候选区域，放入卷积神经网络中进行分类，
卷积神经网络在卷积之后会接上全连接层，将产生的特征矩阵（二维）压缩为一维向量，全连接层最后一层输出概率向量，概率最大的就是当前目标的类别。
但是在语义分割中这么做有明显的缺点：语义分割需要对每一个像素分类，若采用前面的方法， 会比检测问题多的多的候选区域，无疑极度增加了计算量和存储需求；
其次，候选区域的大小不易确定；再者，网络的感知域受到候选区域大小的限制，不能很好地利用上下文信息。</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该论文开创性地提出了一种end-to-end的全卷积网络训练方式，使卷积神经网络方法可以现实的进行语义分割。
</p>

<h4 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
如图1-1所示，FCN相较于CNN的不同之处，是讲网络后面的全连接层替换成为了卷积层，最后一层的输出由一条线（概率向量）变为了一个二维热图（实际上是三维输出，每个像素有属于不同类别的概率，给输出增添了一维）。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-1.png" width="80%" height="80%" div align="center">
<div style="font-size: small; text-align: center">图1-1.FCN与CNN</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   将特征矩阵从粗糙（coarse outputs）变回像素级别的预测(pixel-dense outputs)是关键，这个过程一般称为上采样（upsample），常见的上采样方法有：
转置卷积（反卷积upconv）、插值法（interpolation）、反池化(unpool)，该论文中采用的方法是双线性插值（bilinearly upsample），选择此方法的原因忘了，
也不想在论文中去找，但我估计这种方法的优点是实现起来相对简单，计算量小一些。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
保留CNN的卷积（conv1-conv4）将fc层（full connection,全连接）替换去掉后，对之前的最后一层卷积继续采用卷积操作（记为conv5，卷积核的大小不一定和论文中一样，本人理解可以自己选择适当的大小），
通过控制通道数将得到的特征通道数增加到与原本的全连接层节点数相同的数目，再加一次池化，再采用一次1x1卷积进训练，（这次不改变通道数，可以记为conv6），然后进行1x1卷积，并将输出通道变为像素类别数（类别加背景）。
最后通过上采样将分辨率调整到目标大小（原图分辨率）。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
由于经历了多层卷积操作，最后得到的特征是高语义，低精度的，为了减少反卷积得到的pixelwise prediction的精度损失，我们可以利用不同卷积层提取的出的不同的层级特征。
如图1-2，论文中采用了skip architecture,该结构使粗糙的深层特征与精度高的浅层特征融合，来产生更准确、精细的结果。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-2.png" width="80%" height="80%" div align="center">
<div style="font-size: small; text-align: center">图1-2.FCN的skip structure</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
将最后得到的最小尺寸的的特征图（这里记作mini-feature吧）直接进行步长为32的上采样，被称为FCN-32s；将最后得到的mini-feature进行2x2上采样，
将结果与pool4后得到的feature_map融合（得到的结果记为mini2_-feature吧），再进行步长为16的上采样，被称为FCN-16s；将mini2_feature进行2倍的上采样，
与pool3得到的feature_map融合，再进行步长为8的上采样，得到原图的分辨率，记为FCN-8s。（对特征融合的具体实现过程，我始终有着疑惑：既然不同层pool的通道数不同，那融合之后通道数应该与目标通道数不同，是否该做额外的处理，该如何处理，还没看到论文说，看样子只有看源代码才能解决问题了！）
</p>

<h4 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="(3) 实验结果"></a>(3) 实验结果</h4><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-3.png" width="60%" height="60%" div align="center">
<div style="font-size: small; text-align: center">图1-3的skip structure</div>
emmmm...到了实验这块，论文我就没有好好看，实际上实验也是十分重要的，这篇论文比较详细地给出了模型的构建过程，以后也可以细看看。这篇论文的做的实验太多了，不往博客写了，还是看论文吧。将实验结果总结起来，有如下结论：

<blockquote>
<p>1.调整了AlexNet，VGG16，GoogleNet等3个分类网络，得出backbone使用VGG16的效果最好（那时候还没有resnet和densenet，要不我想结果会改变的）<br>2.FCN-8s的效果好于FCN-16s和FCN-32s<br>3.对整幅图训练和对50%、25%采样后的图训练，最后得到的loss是差不多的，但是对整幅图训练loss下降得快一些。</p>
</blockquote>
<h3 id="2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation"><a href="#2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="2. U-Net:Convolution Networks for Biomedical Image Segmentation"></a>2. U-Net:Convolution Networks for Biomedical Image Segmentation</h3><blockquote>
<p>论文题目：《U-Net:Convolution Networks for Biomedical Image Segmentation》<br>论文地址：<a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>
</blockquote>
<h4 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
在医学图像处理中，常常缺少足够的标记数据 。Ciresan等人训练了一个利用滑动窗口来预测像素类别的分割网络，这种网络通过在像素周围产生一个区域local region（patch）作为输入。
这种网络既能给像素定位，也能产生更多的训练数据。但这种方法有两个明显的缺点，第一：慢；第二：在位置精度与上下文的使用具有矛盾（用自己的话说：高语义低精度和高精度低语义之间的矛盾)。
这篇论文中，在FCN为基础上，提出了U-net。</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet1.png" , align="center" height="80%" width="80%">
<div style="font-size: small;text-align: center">图2-1.论文中U-net的网络图 </div>

<h4 id="2-模型结构-1"><a href="#2-模型结构-1" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
如图2-1所示，相较于FCN一个大的不同，是在上采样中U-net使用了大量的特征通道，在最后一步中才将通道数降为类别数，这样在传播中能够为高语义部分保留更多的上下文信息。
该网络形似一个字母'U',这也是它的名字的由来吧！而且我们可以看到，在expansive path中通过skip connection融合了来自contracting path中对应层级的特征。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
医学图片一般分辨率很大，所以我们往往将其切割成许多小的图片，在切割过程中，为了能让图片边界也能“享有”从分利用上下文信息的权利，论文采用了一种称作“overlap-tile”的方法，如图2-2所示，对原图的边界部分采用了对称策略（镜像拷贝），将原图扩大一些，这样就能增加一些边界部分像素的上下文信息了。
（老师曾布置给我一个遥感图像变化检测任务，对于原分辨率的图像我也是这么处理的，这么一想，我也做了一定程度的创新，嘿嘿）。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet2.png" , align="center" width="80%" height="80%">
<div style="text-align: center;font-size: small">图2-2. Overlap-tile</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
为了一定程度上解决缺乏数据量的问题，可以将图片进行数据增强，除了传统的数据增强方法外，还可以针对医学图像的特殊性质而安排的增强方式：弹性增强。
顾名思义，由于细胞是有弹性的，所以我们可以对图片和其标记做一些弹性的形变，放入网络中进行训练。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
另一个挑战是相同类别的细胞总是接触到了一起，这就让细胞之间的边界难以被识别，论文采用了带权损失，在相互接触的两个细胞之间的连接（边界）部分获得更高的权重，如图2-3所示。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet3.png" , height="80%" width="80%" align="center">
<div style="text-align: center;font-size: small">图2-3</div>

<h4 id="3-实验"><a href="#3-实验" class="headerlink" title="(3) 实验"></a>(3) 实验</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
由于论文较为古老，所以实验结果没有细看，大概就是在各种医学图像分割的结果中，都碾压了当时的其他模型吧。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
U-net模型十分轻量，在图像分割使上具有十分重大的意义，之后的许多模型都是基于U-net进行魔改，确实是为图像分割开创了一个新的篇章。
</p>

<h3 id="3-《deeplabv1》空洞卷积"><a href="#3-《deeplabv1》空洞卷积" class="headerlink" title="3.《deeplabv1》空洞卷积"></a>3.《deeplabv1》空洞卷积</h3><div style="font-size: xxx-large">未完待续！！！</div>


]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>语义分割</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>图像语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexNet论文总结</title>
    <url>/2021/07/19/AlexNet%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>ALexNet可以说是第一个深度神经网络，自2012年AlexNet在ImageNet比赛上获得冠军，卷积神经网络逐渐取代传统算法成为了处理计算机视觉任务的核心。AlexNet可以说是用DL来做CV的开篇之作，所以有必要好好看看这篇论文。</p>
</blockquote>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>当时的目标识别主要还是用传统的机器学习方法，在这篇论文之前用来做训练的数据集还是相对比较小的，在数据集上表现很好的结果却在现实中遇到了很大的不同。
直到最近才有比较大的数据集(例如LabelMe和ImageNet)。<br>
为了能够从数百万个图片中学习识别成千上万种物体，我们需要一个一个学习能力更强的模型。然而，对象识别任务的巨大复杂性意味着这个问题甚至不能通过ImageNet这么大的数据集来指定，所以我们的模型也应该有大量的先验知识来弥补我们没有的所有数据。
卷积神经网络（CNN）的学习能力可以通过调整深度和广度来控制，而且它们也对图像的本质（即统计的平稳性和像素相关性的局部性）做出了强有力且基本正确的假设。因此，与具有相似层数的标准前馈神经网络相比，cnn的连接和参数要少得多，因此更容易训练，而它们在理论上的最佳性能可能只略差一些。<br>
这篇论文的特殊贡献为:
</p>
<ol>
    <li>在ImageNet数据集上训练了当时最大的卷积神经网络，在ILSVRC-2010和ILSVRC-2012竞赛上得到了历史最好结果</li>
    <li>写出了对于2D卷积的在GPU上高速优化的实现</li>
    <li></li>
</ol>

<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>ImageNet数据集有大概一千五百万章图片，22000个类别。ImageNet包含了变分辨率的图片，但是作者的系统需要不变分辨率的输入。
因此，对矩形图像，我们首先对图像缩放，使短边长为256，然后剪出中心的256x256的图片。我们不对图像做任何其他的预处理，除了每个像素减去训练集上的平均活动（mean activity, 这个mean activity是啥。。）
。所以我们在用未加工的RGB图像的像素值训练网络。</p>

<h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><p>网络结构如图2所示。它包含了8层——5层卷积核3层全连接。</p>

<h2 id="Relu非线性函数-Rectified-Linear-Units"><a href="#Relu非线性函数-Rectified-Linear-Units" class="headerlink" title="Relu非线性函数(Rectified Linear Units)"></a>Relu非线性函数(Rectified Linear Units)</h2><p>在这个模型以前神经网络的激活函数通常为$f(x) = tanh(h)$或$f(x) = (1 + e^{-x})^{-1}$,根据梯度下降的训练时间，这些饱和的非线性函数比不饱和激活函数$f(x) = max(0,x)$(relu)慢。
如图1，论文中的这个图展示了ReLU和传统激活函数的速度差异。
</p>
<img src="/2021/07/19/AlexNet%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/img1.png" , width="100%" height="30%" align="center">
<p style="font-size: small;text-align: center">图1. </p>

<h2 id="在多GPU上训练"><a href="#在多GPU上训练" class="headerlink" title="在多GPU上训练"></a>在多GPU上训练</h2><p>作者当时的条件有点艰苦，只有GTX580,3GB的显卡，这限制了大型网络在它上面的训练。聪明的作者把网络设计成了在两张GPU上训练的结构，这两个GPU能够直接读写相互的内存而不通过宿主机的内存。
作者使用的平行模式在每张GPU上放置一半的kernels(或neurons),并有另一个技巧：GPU只在某些层通信。具体在那些层通信也是要经过选择的。
</p>

<h2 id="局部响应标准化（Local-Response-Normalization）"><a href="#局部响应标准化（Local-Response-Normalization）" class="headerlink" title="局部响应标准化（Local Response Normalization）"></a>局部响应标准化（Local Response Normalization）</h2><p>说实话，我现在(2021/1/19,15:51)看到这个词完全是懵的，这是个啥。。继续看看论文吧。。<br>
一般ReLU不需要将输入标准化来防止饱和（例如sigmoid函数在x过大或过小时导数趋近于0），当输入为正值时，那个神经元将发生学习。然而，作者仍然发现了局部标准化能增加泛化能力。
把卷积核i的位置（x,y）的神经元记为$a_{x,y}^{i}$,并对其使用ReLU函数，局部标准化的结果$b_{x,y}^{i}$被定义为:
$$ b_{x,y}^{i} = a_{x,y}^{i} / {\left( k+\alpha {\sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}}{\left( a_{x,y}^{i}\right) }^{2}\right) }^{\beta} $$
现在的模型都不用这个了，故不详细说了，从公式也挺好看明白的。
</p>

<h2 id="重叠的池化"><a href="#重叠的池化" class="headerlink" title="重叠的池化"></a>重叠的池化</h2><p>文章中的实现用到了重叠的池化（Overlapping Pooling），即步长s=2,尺寸z=3，对比s=2,z=2的不重叠的池化，识别错误率会减少0.3%。我想用Overlapping的池化大概会丢失更少的语义信息吧！</p>

<h2 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h2><p>如图2为论文中的模型结构图，网络包含了8层有权重的层：前5层为卷积层，后3层是全连接层。最后一层产生1000路的输出，代表1000类标签的概率分布。
图3是论文中关于网络结构的描述，我就不赘述了。
</p>
<img src="/2021/07/19/AlexNet%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/img2.png" , height="100%" width="100%" align="center" alt="图2">
<p style="text-align: center;font-size: small">图2</p>
<img src="/2021/07/19/AlexNet%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/img3.png" , height="100%" width="100%" align="center" alt="图3">
<p style="font-size: small;text-align: center">图3</p>

<h1 id="减少过拟合"><a href="#减少过拟合" class="headerlink" title="减少过拟合"></a>减少过拟合</h1><br>

<blockquote>
<p>即使ImageNet有非常多的图片了，但是还是不足以防止像ALexNet这么大的模型的过拟合。所以需要一些防止过拟合的办法。</p>
</blockquote>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>第一个方法是从256x256的图片中随机提取224x224的子图片(patch)（包括它们的水平倒影）将他们放入网络中训练，论文中用这种方法多产生了2048倍的图片[256-224=32,32*32=1024,1024*2=2048],
（这么一看不是随机了，而是能抽取的全都抽取了。。）。在测试时，抽取5个224x224的patch(四个角的和中间的，记录他们的水平倒影，这样就有10个图片了)，然后放入网络中预测， 然后对10个预测的结果进行平均（softmax之后的）。
</p>
<p>第二个方法是改变训练数据上RGB通道上的强度。论文对RGB通道数据使用了PCA来拓展数据集（论文说的我看了半天没看懂。。）</p>

<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout是一种非常有效的防止过拟合的方法。每个神经元有一定的概率被隐藏（论文中是0.5）。被隐藏得神经元以某种方式不参与向前和向后的传播。所以每次进行输入后，
神经网络呈现一个不同的结构，但是所有结构是分享权重的。这个技术减少了神经元之间的协同适应，因为一个神经元不能依赖其他神经元的出现。因此，它被迫学习与其他神经元的许多不同随机子集结合使用的更健壮的特征。
在测试的时候，使用所有的神经元，但是把它们的输出乘以0.5，这样做使输出的期望值和dropout之前的期望值一样。作者在全连接层使用了Dropout。Dropout能有效阻止过拟合，Dropout大概使网络收敛地迭代次数增加到2倍。
</p>]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>分类</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
        <tag>分类</tag>
      </tags>
  </entry>
</search>
