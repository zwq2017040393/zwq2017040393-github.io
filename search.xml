<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/07/06/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>试验</tag>
      </tags>
  </entry>
  <entry>
    <title>语义分割经典论文小结</title>
    <url>/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>导读：图像的语义分割是计算机视觉的基本任务之一，旨在将图像中的每一个像素进行分类。相对于传统分隔方法，从FCN开始的以深度学习方法为主的端到端语义分割模型将精度带来了质上的提升，<br>本文将最近看过的语义分割模型进行一个总结，旨在在今后的学习中，能够更快更好得回忆起曾经看过的论文，找到思路。（毕竟只是小结，细节不是那么全面）</p>
</blockquote>
<h3 id="1-开山之作：FCN全卷积神经网络"><a href="#1-开山之作：FCN全卷积神经网络" class="headerlink" title="1. 开山之作：FCN全卷积神经网络"></a>1. 开山之作：FCN全卷积神经网络</h3><blockquote>
<p>论文题目《Fully Convolutional Networks for Semantic Segmentation》<br>论文地址:<a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a><br><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-0.png" width="60%" height="60%" div align="center"></p>
</blockquote>
<h4 id="1-简介"><a href="#1-简介" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在检测任务中，通常对图片产生一定量的候选区域，放入卷积神经网络中进行分类，
卷积神经网络在卷积之后会接上全连接层，将产生的特征矩阵（二维）压缩为一维向量，全连接层最后一层输出概率向量，概率最大的就是当前目标的类别。
但是在语义分割中这么做有明显的缺点：语义分割需要对每一个像素分类，若采用前面的方法， 会比检测问题多的多的候选区域，无疑极度增加了计算量和存储需求；
其次，候选区域的大小不易确定；再者，网络的感知域受到候选区域大小的限制，不能很好地利用上下文信息。</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该论文开创性地提出了一种end-to-end的全卷积网络训练方式，使卷积神经网络方法可以现实的进行语义分割。
</p>

<h4 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
如图1-1所示，FCN相较于CNN的不同之处，是讲网络后面的全连接层替换成为了卷积层，最后一层的输出由一条线（概率向量）变为了一个二维热图（实际上是三维输出，每个像素有属于不同类别的概率，给输出增添了一维）。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-1.png" width="80%" height="80%" div align="center">
<div style="font-size: small; text-align: center">图1-1.FCN与CNN</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   将特征矩阵从粗糙（coarse outputs）变回像素级别的预测(pixel-dense outputs)是关键，这个过程一般称为上采样（upsample），常见的上采样方法有：
转置卷积（反卷积upconv）、插值法（interpolation）、反池化(unpool)，该论文中采用的方法是双线性插值（bilinearly upsample），选择此方法的原因忘了，
也不想在论文中去找，但我估计这种方法的优点是实现起来相对简单，计算量小一些。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
保留CNN的卷积（conv1-conv4）将fc层（full connection,全连接）替换去掉后，对之前的最后一层卷积继续采用卷积操作（记为conv5，卷积核的大小不一定和论文中一样，本人理解可以自己选择适当的大小），
通过控制通道数将得到的特征通道数增加到与原本的全连接层节点数相同的数目，再加一次池化，再采用一次1x1卷积进训练，（这次不改变通道数，可以记为conv6），然后进行1x1卷积，并将输出通道变为像素类别数（类别加背景）。
最后通过上采样将分辨率调整到目标大小（原图分辨率）。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
由于经历了多层卷积操作，最后得到的特征是高语义，低精度的，为了减少反卷积得到的pixelwise prediction的精度损失，我们可以利用不同卷积层提取的出的不同的层级特征。
如图1-2，论文中采用了skip architecture,该结构使粗糙的深层特征与精度高的浅层特征融合，来产生更准确、精细的结果。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-2.png" width="80%" height="80%" div align="center">
<div style="font-size: small; text-align: center">图1-2.FCN的skip structure</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
将最后得到的最小尺寸的的特征图（这里记作mini-feature吧）直接进行步长为32的上采样，被称为FCN-32s；将最后得到的mini-feature进行2x2上采样，
将结果与pool4后得到的feature_map融合（得到的结果记为mini2_-feature吧），再进行步长为16的上采样，被称为FCN-16s；将mini2_feature进行2倍的上采样，
与pool3得到的feature_map融合，再进行步长为8的上采样，得到原图的分辨率，记为FCN-8s。（对特征融合的具体实现过程，我始终有着疑惑：既然不同层pool的通道数不同，那融合之后通道数应该与目标通道数不同，是否该做额外的处理，该如何处理，还没看到论文说，看样子只有看源代码才能解决问题了！）
</p>

<h4 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="(3) 实验结果"></a>(3) 实验结果</h4><img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/FCN-3.png" width="60%" height="60%" div align="center">
<div style="font-size: small; text-align: center">图1-3的skip structure</div>
emmmm...到了实验这块，论文我就没有好好看，实际上实验也是十分重要的，这篇论文比较详细地给出了模型的构建过程，以后也可以细看看。这篇论文的做的实验太多了，不往博客写了，还是看论文吧。将实验结果总结起来，有如下结论：

<blockquote>
<p>1.调整了AlexNet，VGG16，GoogleNet等3个分类网络，得出backbone使用VGG16的效果最好（那时候还没有resnet和densenet，要不我想结果会改变的）<br>2.FCN-8s的效果好于FCN-16s和FCN-32s<br>3.对整幅图训练和对50%、25%采样后的图训练，最后得到的loss是差不多的，但是对整幅图训练loss下降得快一些。</p>
</blockquote>
<h3 id="2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation"><a href="#2-U-Net-Convolution-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="2. U-Net:Convolution Networks for Biomedical Image Segmentation"></a>2. U-Net:Convolution Networks for Biomedical Image Segmentation</h3><blockquote>
<p>论文题目：《U-Net:Convolution Networks for Biomedical Image Segmentation》<br>论文地址：<a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>
</blockquote>
<h4 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="(1) 简介"></a>(1) 简介</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
在医学图像处理中，常常缺少足够的标记数据 。Ciresan等人训练了一个利用滑动窗口来预测像素类别的分割网络，这种网络通过在像素周围产生一个区域local region（patch）作为输入。
这种网络既能给像素定位，也能产生更多的训练数据。但这种方法有两个明显的缺点，第一：慢；第二：在位置精度与上下文的使用具有矛盾（用自己的话说：高语义低精度和高精度低语义之间的矛盾)。
这篇论文中，在FCN为基础上，提出了U-net。</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet1.png" , align="center" height="80%" width="80%">
<div style="font-size: small;text-align: center">图2-1.论文中U-net的网络图 </div>

<h4 id="2-模型结构-1"><a href="#2-模型结构-1" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
如图2-1所示，相较于FCN一个大的不同，是在上采样中U-net使用了大量的特征通道，在最后一步中才将通道数降为类别数，这样在传播中能够为高语义部分保留更多的上下文信息。
该网络形似一个字母'U',这也是它的名字的由来吧！而且我们可以看到，在expansive path中通过skip connection融合了来自contracting path中对应层级的特征。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
医学图片一般分辨率很大，所以我们往往将其切割成许多小的图片，在切割过程中，为了能让图片边界也能“享有”从分利用上下文信息的权利，论文采用了一种称作“overlap-tile”的方法，如图2-2所示，对原图的边界部分采用了对称策略（镜像拷贝），将原图扩大一些，这样就能增加一些边界部分像素的上下文信息了。
（老师曾布置给我一个遥感图像变化检测任务，对于原分辨率的图像我也是这么处理的，这么一想，我也做了一定程度的创新，嘿嘿）。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet2.png" , align="center" width="80%" height="80%">
<div style="text-align: center;font-size: small">图2-2. Overlap-tile</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
为了一定程度上解决缺乏数据量的问题，可以将图片进行数据增强，除了传统的数据增强方法外，还可以针对医学图像的特殊性质而安排的增强方式：弹性增强。
顾名思义，由于细胞是有弹性的，所以我们可以对图片和其标记做一些弹性的形变，放入网络中进行训练。
</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
另一个挑战是相同类别的细胞总是接触到了一起，这就让细胞之间的边界难以被识别，论文采用了带权损失，在相互接触的两个细胞之间的连接（边界）部分获得更高的权重，如图2-3所示。
</p>
<img src="/2021/07/08/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%B0%8F%E7%BB%93/Unet3.png" , height="80%" width="80%" align="center">
<div style="text-align: center;font-size: small">图2-3</div>
<div style="font-size: xxx-large">未完待续！！！</div>


]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>语义分割</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>图像语义分割</tag>
      </tags>
  </entry>
</search>
